{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução\n",
        "\n",
        "Nesse colab vou explicar como treinar um modelo de detecção de objetos YOLOv11 em um PC local, um modelo yolo pode ser treinado em um colab como este também, mas para o desafio explicarei como treinar o yolo localmente.\n",
        "\n",
        "Vou falar sobre o processo de preparação de dados, instalação do Ultralytics(https://docs.ultralytics.com/pt/), treinamento de um modelo e execução de inferencia com um script Python personalizado para o uso da transmissão do hardware ESP32Cam.\n",
        "\n",
        "Estarei mostrando o passo a passo para windows, mas com poucas modificações pode ser usado em sistemas Linux.\n",
        "\n",
        "OBSERVAÇÃO : Uma placa de vídeo NVIDIA compatível com CUDA é altamente recomendada para modelos de treinamento."
      ],
      "metadata": {
        "id": "zQi9uzfDPgW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbCgz7g5M3zk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 1 - Instalar o Anaconda\n",
        "\n",
        "Primeiro, precisamos instalar o Anaconda( https://anaconda.com/download ), uma ótima ferramenta para criar e gerenciar ambientes Python. Ele permite que você instale bibliotecas Python sem se preocupar com conflitos de versão com instalações existentes no seu sistema operacional."
      ],
      "metadata": {
        "id": "H2b569nNS8XG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 2 - Crie um novo ambiente e instale o Ultralytics\n",
        "\n",
        "Agora, vamos criar um novo ambiente Anaconda Python para a biblioteca Ultralytics. Siga as seguintes etapas: Crie um novo ambiente executando:"
      ],
      "metadata": {
        "id": "U0ER3Z0jTsbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conda create --name yolo11-env python=3.12 -y"
      ],
      "metadata": {
        "id": "9Nt36io1T9Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conda activate yolo11-env"
      ],
      "metadata": {
        "id": "SyX4mcexT_a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics"
      ],
      "metadata": {
        "id": "y_6_yDo9UB5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao instalar o Ultralytics, o gerenciador de pacotes Python também instala diversas outras bibliotecas poderosas, como OpenCV, Numpy e PyTorch."
      ],
      "metadata": {
        "id": "0iGHfhT0UH5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para executar o treinamento na GPU, precisamos usar a versão do PyTorch habilitada para GPU. Execute o seguinte comando para instalar a versão do PyTorch para GPU:"
      ],
      "metadata": {
        "id": "DdVYOSDWUSVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVAÇÃO: O PyTorch lança regularmente versões atualizadas com suporte para versões mais recentes do CUDA. O comando acima instala o PyTorch com o CUDA v12.4. Versão mais recente em ( https://pytorch.org/get-started/locally/ )"
      ],
      "metadata": {
        "id": "FzDGv_UCUehp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "id": "7uDaN0jaUJE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 3 - Reúna e rotule as imagens\n",
        "\n",
        "O primeiro passo para treinar qualquer modelo de aprendizado de máquina é construir um conjunto de dados. Para modelos de detecção de objetos, isso significa coletar e rotular centenas a milhares de imagens dos objetos que você deseja detectar.\n",
        "\n",
        "onjuntos de dados podem ser coletados online a partir de dados preexistentes em sites como Kaggle(https://www.kaggle.com/) , Open Images V7(https://storage.googleapis.com/openimages/web/index.html) ou Roboflow Universe (https://universe.roboflow.com/).\n",
        "\n",
        "Normalmente eu usaria o roboflow para construção e categorização dos conjuntos de dados para o treinamento, mas como tratamos de um desafio industrial, acredito que o roblow por mais que seja facil, não garante segurança e controle de dados (além de conjutos de dados capturados manualmente tendem a ter uma melhor performace no desempenho). pontos que são muito importantes para empresas em geral. por isso optei por usar o Label Studio (https://github.com/HumanSignal/labelImg)\n",
        "\n",
        "Existem duas **etapas principais** para desenvolver um conjunto de dados de treinamento personalizado eficaz para modelos de detecção de objetos:\n",
        "\n",
        "1. Coletando imagens\n",
        "2. Rotulagem de objetos"
      ],
      "metadata": {
        "id": "rClyY_hJUThv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U label-studio"
      ],
      "metadata": {
        "id": "NxojS214Vjko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label-studio"
      ],
      "metadata": {
        "id": "LNX4ibF4Xu53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apos fazer a instalação do aplicativo do label studio e executa-lo ele será aberto para carregar e classificar as imagens."
      ],
      "metadata": {
        "id": "DSnqHrZ9WePs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Então export os dados no formato YOLO, e tera um arquivo .zip\n",
        "\n",
        "- Uma pasta “imagens” contendo as imagens\n",
        "- Uma pasta “labels” contendo os rótulos no formato de anotação YOLO\n",
        "- Um arquivo labelmap “classes.txt” que contém a lista de classes"
      ],
      "metadata": {
        "id": "bafIYcl_YJRa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZ_W9-QQidpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 4 - Configurar estrutura de pastas\n",
        "\n",
        "O Ultralytics requer uma estrutura de pasta específica para armazenar dados de treinamento para modelos. A pasta raiz é chamada de \"dados\". Dentro dela, há duas pastas principais:\n",
        "\n",
        "\n",
        "- train : Contém as imagens e rótulos que são fornecidos ao modelo durante o processo de treinamento.\n",
        "- Validação : Contém imagens e rótulos que são usados ​​periodicamente para testar o modelo durante o treinamento. Ao final de cada período de treinamento, o modelo é executado nessas imagens para determinar métricas como precisão, recall e mAP.\n",
        "\n",
        "80% das imagens em um conjunto de dados são divididas na pasta \"train\", enquanto 20% são divididas na pasta \"validation\""
      ],
      "metadata": {
        "id": "ghozedFeY3_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 5 - Configurar treinamento\n",
        "\n",
        "Precisamos criar o arquivo data.yaml de configuração de treinamento do Ultralytics."
      ],
      "metadata": {
        "id": "gppjJpUkZroH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# algo do tipo\n",
        "\n",
        "path: C:\\Users\\<username>\\Documents\\yolo\\data\n",
        "train: train\\images\n",
        "val: validation\\images\n",
        "\n",
        "nc: 2\n",
        "\n",
        "names: [\"Milho\", \"Erva Daninha\"]"
      ],
      "metadata": {
        "id": "FuVbeuRqbFat"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 6 - Treine o modelo!\n",
        "\n",
        "Há uma variedade de tamanhos de modelos disponíveis para treinamento. (https://docs.ultralytics.com/pt/models/yolo11/)\n",
        "\n",
        "Os **parâmetros importantes a serem considerados** são mAP, parâmetros e FLOPs. O valor de mAP indica a precisão relativa do modelo (ou seja, sua eficiência na detecção de objetos) em comparação com os outros modelos. As colunas parâmetros e FLOPs indicam o tamanho do modelo: quanto maiores esses valores, mais recursos computacionais o modelo exigirá.\n",
        "\n",
        "Os modelos YOLO são normalmente treinados e inferidos com **uma resolução de 640x640**. A resolução de entrada do modelo pode ser ajustada usando o argumento \"–imgsz\" durante o treinamento. A resolução do modelo tem um grande impacto na velocidade e precisão do modelo: um modelo com resolução mais baixa terá maior velocidade, mas menor precisão.\n",
        "\n",
        "Em aprendizado de máquina, **uma \"época\"** é uma única passagem pelo conjunto de dados de treinamento completo. Em cada época, todas as imagens do conjunto de dados são inseridas no modelo, e o algoritmo de aprendizado atualiza os pesos internos do modelo para melhor se adequarem aos dados na imagem. A melhor quantidade de épocas a ser usada depende do tamanho do conjunto de dados.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0U3mOHQaj6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treinando o modelo**\n",
        "\n",
        "O comando “yolo detect train” é usado para executar o treinamento. Ele possui alguns argumentos importantes:\n",
        "\n",
        "- **dados** : especifica o caminho para o arquivo de configuração de treinamento (que configuramos na Etapa 5)\n",
        "- **modelo** : especifica qual arquitetura de modelo treinar (ex.: \"yolo11s.pt\", \"yolo11l.pt\"). Você também pode treinar os modelos YOLOv5 e YOLOv8 substituindo \"yolo11\" por \"yolov5\" ou \"yolov8\" (ex.: \"yolov5l.pt\" ou \"yolov8s.pt\").\n",
        "- **épocas** : define o número de épocas para treinar\n",
        "- **imgsz** : define a dimensão de entrada (ou seja, resolução) do modelo YOLO"
      ],
      "metadata": {
        "id": "DJgpdP_LcqdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo detect train data=data.yaml model=yolo11s.pt epochs=60 imgsz=640"
      ],
      "metadata": {
        "id": "61nod9QMbhb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo de treinamento analisará as imagens nos diretórios de treinamento e validação e, em seguida, iniciará o treinamento do modelo.\n",
        "\n",
        "O treinamento terminará ao atingir o número de períodos especificado por \"epochs\"."
      ],
      "metadata": {
        "id": "JYw6emqjdFMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Etapa 7 - Executar modelo\n",
        "\n",
        "Depois de terminar o treinamento do modelo, vamos usar o treinamento para capturar as imagens do endpoint da camera do hardware do ESP32Cam.\n",
        "\n",
        "\n",
        "Aqui vou deixar um codigo de exemplo da aplicação para uso."
      ],
      "metadata": {
        "id": "NjhfhQFXel1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "\n",
        "# Carregar modelo YOLO\n",
        "model_path = \"runs/detect/train/weights/best.pt\"  # Substitua pelo seu caminho real\n",
        "model = YOLO(model_path)\n",
        "labels = model.names\n",
        "\n",
        "# Definir cores para as bounding boxes\n",
        "bbox_colors = [(164,120,87), (68,148,228), (93,97,209), (178,182,133), (88,159,106),\n",
        "               (96,202,231), (159,124,168), (169,162,241), (98,118,150), (172,176,184)]\n",
        "\n",
        "# Loop de inferência\n",
        "while True:\n",
        "    try:\n",
        "        # Pegar frame do endpoint /stream\n",
        "        response = requests.get(\"http://127.0.0.1:5002/stream\", stream=True)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Erro ao acessar o stream.\")\n",
        "            break\n",
        "\n",
        "        img_array = np.asarray(bytearray(response.content), dtype=np.uint8)\n",
        "        frame = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if frame is None:\n",
        "            print(\"Frame inválido.\")\n",
        "            continue\n",
        "\n",
        "        # Início da contagem de tempo\n",
        "        t_start = time.perf_counter()\n",
        "\n",
        "        # Rodar inferência\n",
        "        results = model(frame, verbose=False)\n",
        "        detections = results[0].boxes\n",
        "\n",
        "        object_count = 0\n",
        "\n",
        "        for i in range(len(detections)):\n",
        "            xyxy = detections[i].xyxy.cpu().numpy().squeeze().astype(int)\n",
        "            xmin, ymin, xmax, ymax = xyxy\n",
        "            classidx = int(detections[i].cls.item())\n",
        "            classname = labels[classidx]\n",
        "            conf = detections[i].conf.item()\n",
        "\n",
        "            if conf > 0.5:\n",
        "                color = bbox_colors[classidx % 10]\n",
        "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "                label = f'{classname}: {int(conf*100)}%'\n",
        "                cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "                object_count += 1\n",
        "\n",
        "        # Calcular FPS\n",
        "        t_stop = time.perf_counter()\n",
        "        fps = 1 / (t_stop - t_start)\n",
        "        cv2.putText(frame, f'FPS: {fps:.2f}', (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
        "        cv2.putText(frame, f'Objetos: {object_count}', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
        "\n",
        "        # Mostrar frame\n",
        "        cv2.imshow(\"YOLO Stream Detection\", frame)\n",
        "        key = cv2.waitKey(1)\n",
        "        if key == ord('q'):\n",
        "            break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro durante a execução: {e}\")\n",
        "        break\n",
        "\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "CeIECdoNdBoe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}